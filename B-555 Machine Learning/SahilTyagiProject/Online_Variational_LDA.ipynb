{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Variational LDA for topic modeling books from gutenburg documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import random\n",
    "\n",
    "seed_val = 1234567\n",
    "np.random.seed(seed_val)\n",
    "random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_preprocess():\n",
    "    \n",
    "    book_ids = {}\n",
    "    ctr = 0\n",
    "    per_book_word_count = {}\n",
    "    global_unique_words = []\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    topics_map = {}\n",
    "    \n",
    "    f = open(current_dir + '/frequent_topics.txt', 'r')\n",
    "    for line in f:\n",
    "        topics_map[ctr] = line.split()\n",
    "        ctr += 1\n",
    "        \n",
    "    f.close()\n",
    "    print('# of topics chosen is ' + str(len(topics_map)))\n",
    "    \n",
    "    ctr = 0\n",
    "    book_dir = os.listdir(current_dir + '/books/')\n",
    "    for book in book_dir:\n",
    "        if '.txt' in book:\n",
    "            f = open(current_dir + '/books/' + book, 'r')\n",
    "        \n",
    "            book_ids[ctr] = book\n",
    "            ctr += 1\n",
    "        \n",
    "            word_count = {}\n",
    "        \n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    if word in word_count.keys():\n",
    "                        word_count[word] = word_count[word] + 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "                    \n",
    "                    if word not in global_unique_words:\n",
    "                        global_unique_words.append(word)\n",
    "            \n",
    "            f.close()\n",
    "            per_book_word_count[book] = word_count\n",
    "        \n",
    "    doc_word_matrix = np.empty([1, len(global_unique_words)])\n",
    "    print('init doc_word_matrix shape ' + str(doc_word_matrix.shape))\n",
    "    print('length of book_ids map is ' + str(len(book_ids)))\n",
    "    #print(book_ids)\n",
    "    \n",
    "    for ix in range(0, len(book_ids)):\n",
    "        book = book_ids[ix]\n",
    "        doc_word_vector = []\n",
    "        word_count = per_book_word_count[book]\n",
    "        for word in global_unique_words:\n",
    "            if word in word_count.keys():\n",
    "                doc_word_vector.append(word_count[word])\n",
    "            else:\n",
    "                doc_word_vector.append(0)\n",
    "                \n",
    "        \n",
    "        doc_word_vector = np.array(doc_word_vector)\n",
    "        doc_word_vector = np.transpose(doc_word_vector).reshape((len(global_unique_words),1))\n",
    "        #print('doc_word_vector shape is ' + str(doc_word_vector.shape))\n",
    "        doc_word_matrix = np.vstack((doc_word_matrix, np.transpose(doc_word_vector)))\n",
    "    \n",
    "    #print(doc_word_matrix.shape)\n",
    "    doc_word_matrix = np.delete(doc_word_matrix, (0), axis=0)\n",
    "    print('final doc_word_matrix shape ' + str(doc_word_matrix.shape))\n",
    "    \n",
    "    return book_ids, topics_map, global_unique_words, doc_word_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of topics chosen is 100\n",
      "init doc_word_matrix shape (1, 16128)\n",
      "length of book_ids map is 10\n",
      "final doc_word_matrix shape (10, 16128)\n"
     ]
    }
   ],
   "source": [
    "book_ids, topics_map, global_unique_words, doc_word_matrix = init_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n",
      "(10, 100)\n",
      "beta_lambda shape is (100, 16128)\n"
     ]
    }
   ],
   "source": [
    "# returns shape as (documents x topics)\n",
    "# numbers of topics\n",
    "K = len(topics_map)\n",
    "# number of documents\n",
    "D = len(book_ids)\n",
    "# size of vocabulary\n",
    "V = len(global_unique_words)\n",
    "#print(V)\n",
    "eta = 0.01\n",
    "# previous_gamma_tk = np.ones((D, K))*0.1\n",
    "previous_gamma_tk = np.ones((D, K))\n",
    "print(previous_gamma_tk.shape)\n",
    "\n",
    "updated_gamma_tk = np.ones((D, K))\n",
    "print(updated_gamma_tk.shape)\n",
    "\n",
    "beta_lambda = np.random.gamma(1.0, 1.0, (K, V)) * (D*100)/(K*V)\n",
    "print('beta_lambda shape is ' + str(beta_lambda.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the Expected values of logtheta and logbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_digamma(lda_matrix, v_index, parameter):\n",
    "    lda_array = lda_matrix[v_index,:]\n",
    "    if parameter == 'beta':\n",
    "        lda_array = lda_array.reshape((1,V))\n",
    "        expected_logtheta_or_logbeta = special.psi(lda_array) - special.psi(np.sum(lda_array))\n",
    "        expected_logtheta_or_logbeta = expected_logtheta_or_logbeta.reshape((1,V))\n",
    "        \n",
    "    elif parameter == 'gamma':\n",
    "        lda_array = lda_array.reshape((1,K))\n",
    "        expected_logtheta_or_logbeta = special.psi(lda_array) - special.psi(np.sum(lda_array))\n",
    "        expected_logtheta_or_logbeta = expected_logtheta_or_logbeta.reshape((1,K))\n",
    "    \n",
    "    return expected_logtheta_or_logbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirichlet parameter for topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "process_gammas = np.zeros((2,K-1))\n",
    "process_gammas[0] = 1.0\n",
    "process_gammas[1] = 0.01\n",
    "\n",
    "gamma_to_use = process_gammas[0]/(process_gammas[0] + process_gammas[1])\n",
    "dirichlet_alpha = float(5)/float(K)\n",
    "print(gamma_to_use.shape)\n",
    "alpha = np.zeros(K)\n",
    "multiplier = 1.0\n",
    "for i in range(0, K-1):\n",
    "    alpha[i] = gamma_to_use[i]*multiplier\n",
    "    multiplier = multiplier - alpha[i]\n",
    "    \n",
    "alpha[K-1] = multiplier\n",
    "alpha = alpha * dirichlet_alpha\n",
    "print(alpha.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current book being processed is The Watsons: By Jane Austen and Concluded by L. Oulton.txt\n",
      "0.4715286515174943\n",
      "0.032389236720794\n",
      "0.0003086718318158521\n",
      "2.7757130396413034e-06\n",
      "current book being processed is Northanger Abbey.txt\n",
      "1.945213813000269\n",
      "0.02100309941837507\n",
      "1.1244997754582542e-05\n",
      "current book being processed is Sense and Sensibility.txt\n",
      "9.239686517506806\n",
      "0.9875579220418303\n",
      "0.0009892959691194907\n",
      "1.2120810914518998e-05\n",
      "current book being processed is Mansfield Park.txt\n",
      "2.6962035629637895\n",
      "0.001637183055124618\n",
      "1.21438297538905e-05\n",
      "current book being processed is Persuasion.txt\n",
      "1.0931089691303477\n",
      "0.004035603476229193\n",
      "1.3090033796666844e-05\n",
      "current book being processed is Emma.txt\n",
      "0.20005162126644963\n",
      "1.7026207724335053e-05\n",
      "current book being processed is Pride and Prejudice.txt\n",
      "1.829586504966365\n",
      "0.3169595010803995\n",
      "0.0025858709048128363\n",
      "3.488775637092445e-05\n",
      "current book being processed is The Letters of Jane Austen.txt\n",
      "2.091982630479664\n",
      "0.0036326601579804423\n",
      "3.508704274090135e-05\n",
      "current book being processed is Lady Susan.txt\n",
      "2.0998872653582823\n",
      "0.0006655528367613285\n",
      "3.509494542533176e-05\n",
      "current book being processed is Love and Freindship.txt\n",
      "0.10008579005263518\n",
      "3.6894792106477946e-05\n"
     ]
    }
   ],
   "source": [
    "# K is the number of topics\n",
    "#take 2.0\n",
    "K = len(topics_map)\n",
    "max_iter = 500\n",
    "for ix in range(0, D):\n",
    "    book = book_ids[ix]\n",
    "    print('current book being processed is ' + str(book))\n",
    "    #delta = np.sum(np.absolute(updated_gamma_tk - previous_gamma_tk))\n",
    "    itr = 0\n",
    "    \n",
    "    doc_wordcount_vec = doc_word_matrix[ix,:]\n",
    "    #print('doc wordcount vect shape ' + str(doc_wordcount_vec.shape))\n",
    "    \n",
    "    expected_logbeta = expectation_digamma(beta_lambda, ix, 'beta')\n",
    "    expo_logbeta = np.exp(expected_logbeta)\n",
    "    expo_logbeta = expo_logbeta.reshape((V,1))\n",
    "    #print('exponent logbeta ' + str(expo_logbeta.shape))\n",
    "    expected_logtheta = expectation_digamma(updated_gamma_tk, ix, 'gamma')\n",
    "    expo_logtheta = np.exp(expected_logtheta)\n",
    "    expo_logtheta = expo_logtheta.reshape((K,1))\n",
    "    phi_dwk = np.dot(expo_logtheta, np.transpose(expo_logbeta)) + 1e-100\n",
    "    #print('exponent logtheta ' + str(expo_logtheta.shape))\n",
    "    #print('exponent logbeta ' + str(expo_logbeta.shape))\n",
    "    #print('exponent phi_dwk ' + str(phi_dwk.shape))\n",
    "    alpha = alpha.reshape((K,1))\n",
    "    #print('alpha shape ' + str(alpha.shape))\n",
    "    \n",
    "    while itr < max_iter:\n",
    "        previous_gamma_tk[ix,:] = updated_gamma_tk[ix,:]\n",
    "        #print('OLD gamma...')\n",
    "        #print(previous_gamma_tk[ix,:])\n",
    "        \n",
    "        term1 = expo_logtheta * np.dot(doc_wordcount_vec/phi_dwk, expo_logbeta)\n",
    "        #print('term1 shape is ' + str(term1.shape))\n",
    "        updated_gamma_tk[ix,:] = np.transpose(alpha + term1)\n",
    "        #print('updated_gamma_tk shape is ' + str(updated_gamma_tk.shape))\n",
    "        expected_logtheta = expectation_digamma(updated_gamma_tk, ix, 'gamma')\n",
    "        expo_logtheta = np.exp(expected_logtheta)\n",
    "        expo_logtheta = expo_logtheta.reshape((K,1))\n",
    "        #print('exponent logtheta ' + str(expo_logtheta.shape))\n",
    "        #print(expo_logtheta)\n",
    "        \n",
    "        phi_dwk = np.dot(expo_logtheta, np.transpose(expo_logbeta)) + 1e-100\n",
    "        #print('phi_dwk shape ' + str(phi_dwk.shape))\n",
    "        \n",
    "        #prod_phi_wordcount = np.sum(np.dot(doc_wordcount_vec, np.transpose(phi_dwk)))\n",
    "        \n",
    "        #updated_gamma_tk[ix,:] = alpha + prod_phi_wordcount\n",
    "        #print('updated_gamma_tk shape is ' + str(updated_gamma_tk.shape))\n",
    "        #print('NEW gamma...')\n",
    "        #print(updated_gamma_tk[ix,:])\n",
    "        \n",
    "        itr += 1\n",
    "        errorchange = np.mean(abs(updated_gamma_tk - previous_gamma_tk))\n",
    "        print(errorchange)\n",
    "        if itr % 50 == 0:\n",
    "            print('error is ' + str(errorchange))\n",
    "        \n",
    "        if (errorchange < 0.0001):\n",
    "            break\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOTHER TRIAL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "alpha = float(1)/float(K)\n",
    "alpha = alpha*np.ones(K)\n",
    "alpha = alpha.reshape(1,K)\n",
    "print(alpha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Watsons: By Jane Austen and Concluded by L. Oulton.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Northanger Abbey.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Sense and Sensibility.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Mansfield Park.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Persuasion.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Emma.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Pride and Prejudice.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "The Letters of Jane Austen.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Lady Susan.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n",
      "Love and Freindship.txt\n",
      "doc wordcount vect shape (16128,)\n",
      "exponent logbeta (16128, 1)\n",
      "delta is 0.9899999999999999\n",
      "delta is 0.0\n",
      "reached decent convergence so gonna exit now...\n"
     ]
    }
   ],
   "source": [
    "# K is the number of topics\n",
    "K = len(topics_map)\n",
    "max_iter = 500\n",
    "for ix in range(0, D):\n",
    "    book = book_ids[ix]\n",
    "    print(book)\n",
    "    #delta = np.sum(np.absolute(updated_gamma_tk - previous_gamma_tk))\n",
    "    itr = 0\n",
    "    \n",
    "    doc_wordcount_vec = doc_word_matrix[ix,:]\n",
    "    print('doc wordcount vect shape ' + str(doc_wordcount_vec.shape))\n",
    "    \n",
    "    expected_logbeta = expectation_digamma(beta_lambda, ix, 'beta')\n",
    "    expo_logbeta = np.exp(expected_logbeta)\n",
    "    expo_logbeta = expo_logbeta.reshape((V,1))\n",
    "    print('exponent logbeta ' + str(expo_logbeta.shape))\n",
    "    \n",
    "    while itr < max_iter:\n",
    "        #print('OLD gamma...')\n",
    "        #print(previous_gamma_tk[ix,:])\n",
    "        expected_logtheta = expectation_digamma(previous_gamma_tk, ix, 'gamma')\n",
    "        \n",
    "        expo_logtheta = np.exp(expected_logtheta)\n",
    "        expo_logtheta = expo_logtheta.reshape((K,1))\n",
    "        #print('exponent logtheta ' + str(expo_logtheta.shape))\n",
    "        #print(expo_logtheta)\n",
    "        \n",
    "#         phi_dwk = np.dot(expo_logtheta, np.transpose(expo_logbeta))\n",
    "        phi_dwk = np.dot(expo_logtheta, np.transpose(expo_logbeta)) + 1e-100\n",
    "        #print('phi_dwk shape ' + str(phi_dwk.shape))\n",
    "        \n",
    "        prod_phi_wordcount = np.sum(np.dot(doc_wordcount_vec, np.transpose(phi_dwk)))\n",
    "        updated_gamma_tk[ix,:] = alpha + prod_phi_wordcount\n",
    "        #print('updated_gamma_tk shape is ' + str(updated_gamma_tk.shape))\n",
    "        #print('NEW gamma...')\n",
    "        #print(updated_gamma_tk[ix,:])\n",
    "        \n",
    "        #delta = float(np.sum(np.absolute(updated_gamma_tk[ix,:] - previous_gamma_tk[ix,:])))/float(K)\n",
    "        delta = np.sum(np.absolute(updated_gamma_tk[ix,:] - previous_gamma_tk[ix,:]))/K\n",
    "        \n",
    "        #test = np.absolute(updated_gamma_tk[ix,:] - previous_gamma_tk[ix,:])\n",
    "        #print(test)\n",
    "        \n",
    "        itr += 1\n",
    "        \n",
    "        print('delta is ' + str(delta))\n",
    "        if delta < 0.00001:\n",
    "            print('reached decent convergence so gonna exit now...')\n",
    "            break\n",
    "            \n",
    "        previous_gamma_tk[ix,:] = updated_gamma_tk[ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
