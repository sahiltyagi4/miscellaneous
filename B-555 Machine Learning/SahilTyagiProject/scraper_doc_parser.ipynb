{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sahiltyagi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sahiltyagi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sahiltyagi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARSING JANE AUSTEN BOOKS FROM GUTENBERG DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did the book Persuasion so far..\n",
      "did the book The Letters of Jane Austen so far..\n",
      "did the book Pride and Prejudice so far..\n",
      "did the book Emma so far..\n",
      "did the book Sense and Sensibility so far..\n",
      "did the book Northanger Abbey so far..\n",
      "did the book Mansfield Park so far..\n",
      "did the book The Watsons: By Jane Austen and Concluded by L. Oulton so far..\n",
      "did the book Lady Susan so far..\n",
      "did the book Love and Freindship so far..\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#removing pronouns, prepositions, conjunctions and interjections, verbs and adverbs\n",
    "what_to_filter_out = ['CC','EX','IN','PRP','PRP$','UH','WDT','WP','WP$','WRB','VB','VBG','VBD','VBP','VBZ','DT','EX','.','RB']\n",
    "\n",
    "jane_austen_homepage = {}\n",
    "file = open(os.getcwd() + '/jane_austen.txt', 'r')\n",
    "for line in file:\n",
    "    jane_austen_homepage[str(line.split(',')[1])] = str(line.split(',')[0])\n",
    "\n",
    "file.close()\n",
    "\n",
    "for url in jane_austen_homepage.keys():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    html = urllib.request.urlopen(url)\n",
    "    f = html.read()\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    book = []\n",
    "    tags = soup.find_all('p')\n",
    "    for tag in tags:\n",
    "        para = ''\n",
    "        word_tokens = word_tokenize(str(tag.get_text()))\n",
    "        for word in word_tokens:\n",
    "            if word not in stop_words and 'CHAPTER' not in word:\n",
    "                para = para + str(word) + ' '\n",
    "        \n",
    "        #tokenizer.tokenize(para)\n",
    "        for w in tokenizer.tokenize(para):\n",
    "            filter_words = word_tokenize(w)\n",
    "            tagged_words = nltk.pos_tag(filter_words)\n",
    "            # to remove all 's' from the contractions like it's, I'm etc... \n",
    "            if tagged_words[0][1] not in what_to_filter_out and tagged_words[0][0] != 's':\n",
    "                book.append(str(tagged_words[0][0]))\n",
    "            \n",
    "    file = open(os.getcwd() + '/books/' + jane_austen_homepage[url] + '.txt', 'w')\n",
    "    for line in book:\n",
    "        file.write(line + ' ')\n",
    "        \n",
    "    file.close()\n",
    "    print('did the book ' + str(jane_austen_homepage[url]) + ' so far..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE WORD FREQUENCY FROM EACH BOOK TO GET SOME IDEAS FOR TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sahiltyagi/Desktop/gutenberg/books/The Watsons: By Jane Austen and Concluded by L. Oulton.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Northanger Abbey.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Sense and Sensibility.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Mansfield Park.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Persuasion.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Emma.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Pride and Prejudice.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/The Letters of Jane Austen.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Lady Susan.txt\n",
      "/Users/sahiltyagi/Desktop/gutenberg/books/Love and Freindship.txt\n"
     ]
    }
   ],
   "source": [
    "books_dir = os.getcwd() + '/books/'\n",
    "files = os.listdir(books_dir)\n",
    "for file in files:\n",
    "    if '.txt' in file:\n",
    "        word_count = {}\n",
    "        print(books_dir + file)\n",
    "        f = open(books_dir + '/' + file, 'r')\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                if word in word_count.keys():\n",
    "                    word_count[word] = word_count[word] + 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "                    \n",
    "        f.close()\n",
    "        all_counts = list(word_count.values())\n",
    "        all_counts.sort(reverse=True)\n",
    "        \n",
    "        K = 200\n",
    "        top_K = all_counts[0:K]\n",
    "        \n",
    "        popular_words = []\n",
    "        for k,v in word_count.items():\n",
    "            if v in top_K:\n",
    "                popular_words.append(k)\n",
    "                \n",
    "        topic_file = open(os.getcwd() + '/top_K_topics/' + file, 'w')\n",
    "        for topic in popular_words:\n",
    "            topic_file.write(topic + '\\n')\n",
    "            \n",
    "        topic_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perhaps\n",
      "RB\n"
     ]
    }
   ],
   "source": [
    "ss = word_tokenize('perhaps')\n",
    "tw = nltk.pos_tag(ss)\n",
    "print(tw[0][0])\n",
    "print(tw[0][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonversion3",
   "language": "python",
   "name": "pythonversion3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
